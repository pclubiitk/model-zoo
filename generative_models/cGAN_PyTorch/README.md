# Pytorch Implementation of Conditional GAN 
### Usage
```bash
$ python3 main.py --ndata 'cifar10' --channel 3 --epochs 100
```
NOTE: on Colab Notebook use following command:
```python
!git clone link-to-repo
%run main.py --ndata 'cifar10' --channel 3 --epochs 100 
```
### Reference
https://arxiv.org/abs/1411.1784

# Summary 

## Introduction

Generative adversarial nets were recently introduced as a novel way to train a generative model.
They consists of two ‘adversarial’ models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training
data rather than G. Both G and D could be a non-linear mapping function, such as a multi-layer perceptron.

## Minimax

Minimax is a decision rule for minimizing the possible loss for a worst case (maximum loss) scenario. 
The maximizer tries to get the highest score possible while the minimizer tries to do the opposite and get the lowest score possible.

Under the game theory perspective, GAN can be viewed as a game of two players: the discriminator D and the generator G. 
The discriminator tries to discriminate the generated (or fake) data and the real data, while the generator attempts to make the discriminator
confusing by gradually generating the fake data that break into the real data. 

## Conditional GAN

Generative adversarial nets can be extended to a conditional model if both the generator and discriminator are conditioned on some extra information y. We can perform the conditioning by feeding y into the both the discriminator and generator as additional input layer. 
In cGAN (Conditional GAN), labels act as an extension to the latent space z to generate and discriminate images better. 
Its structure can be shown as:

![GAN](https://golden-storage-production.s3.amazonaws.com/topic_images/23a36a66d85947c7a0fe4a2ced52914e.png)

To learn a generator distribution Pg over data data x conditoned on label y, the generator builds a mapping function from
a prior noise distribution Pz(z) to data space as G(Z|Y). And the discriminator, D(X|Y), outputs
a single scalar representing the probability that x came form training data rather than Pg.

G and D are both trained simultaneously: we adjust parameters for G to minimize log(1 − D(G(Z|Y))
and adjust parameters for D to minimize log D(X|Y), as if they are following the two-player min-max
game with value function V(G, D):

![Equation](https://miro.medium.com/max/1400/1*l2tSqFN0Afwizm4LgalCGg.png)

One the intuition behing Condional GANs is that, a typical GAN would operate with a generator G(z) where z is a random vector and a discriminator D(G(z)), where as a conditional GAN would add additional information to both discriminator and generator eventually leading to a better performance.

## Implementaion

We simply give conditioning input and prior noise as inputs to the MLP architecture. We use Sigmoid, Relu and Maxout activations in the layers of our MLP. 

- We train our model for 100 epoch 
- We use BCE loss(Binary Crossentropy loss) with a learning rate of 0.0002
- We test the model by generating images for a fixed randomly generated noise and label

## Results

## On FastionMNIST Dataset

Here is the loss graph
> **_NOTE:_** In this graph blue plot corresponds to generator loss and orange to discriminator loss

![graph](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-fmnist/plot%20(1).png)

Here is the gif generated by the images generated by the fixed noise and label.

![gif](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-fmnist/fmnist.gif)

Pretrained model of [generator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-fmnist/generator_100_%20(1).pth) and [discriminator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-fmnist/discriminator_100_%20(1).pth)

## On CIFAR10 Dataset

Here is the loss graph
> **_NOTE:_** In this graph blue plot corresponds to generator loss and orange to discriminator loss

![graph](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-cifar10/plot%20(2).png)

Here is the gif generated by the images generated by the fixed noise and label.

![gif](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-cifar10/cifar10.gif)

Pretrained model of [generator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-cifar10/generator_100_%20.pth) and [discriminator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-cifar10/discriminator_100_%20.pth)

## On MNIST Dataset

Here is the loss graph
> **_NOTE:_** In this graph blue plot corresponds to generator loss and orange to discriminator loss

![graph](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-mnist/plot%20(3).png)

Here is the gif generated by the images generated by the fixed noise and label.

![gif](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-mnist/mnist%20(2).gif)

Pretrained model of [generator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-mnist/generator_100_%20(2).pth) and [discriminator](https://github.com/ayush12gupta/model-zoo/blob/master/generative_models/cGAN_PyTorch/Result-mnist/discriminator_100_%20(2).pth)
