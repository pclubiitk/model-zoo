# Pytorch Implementation of StackGAN

## Usage

Download the images from [OXFORD 102 dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/)

Download the preprocessed char-CNN-RNN text embeddings of [flowers](https://drive.google.com/file/d/0B3y_msrWZaXLaUc0UXpmcnhaVmM/view?usp=drive_open)

```bash
$ python3 main.py --data_dir 'path to embeddings data folder' --image_dir 'path to folder of image folder'
```

> **_NOTE:_** on Colab Notebook use following command:
```python
!git clone link-to-repo
%run main.py --data_dir 'path to embeddings data folder' --image_dir 'path to folder of image folder'
```
### Help Log
```
usage: main.py [-h] [--image_size1 IMAGE_SIZE1] [--image_size2 IMAGE_SIZE2]
               [--z_dim Z_DIM] [--embed_dim EMBED_DIM] [--D1_dim D1_DIM]
               [--G2_dim G2_DIM] --data_dir DATA_DIR --image_dir IMAGE_DIR
               [--epochs1 EPOCHS1] [--epochs2 EPOCHS2]
               [--batch_size BATCH_SIZE] [--lrG1 LRG1] [--lrG2 LRG2]
               [--lrD1 LRD1] [--lrD2 LRD2] [--beta1 BETA1] [--beta2 BETA2]

optional arguments:
  -h, --help            show this help message and exit
  --image_size1 IMAGE_SIZE1
                        stage1 image size
  --image_size2 IMAGE_SIZE2
                        stage2 image size
  --z_dim Z_DIM         noise dimension
  --embed_dim EMBED_DIM
                        embedding compressed dim
  --D1_dim D1_DIM
  --G2_dim G2_DIM
  --data_dir DATA_DIR   path to embedding directory containing pickle files
  --image_dir IMAGE_DIR
                        path to folder containing jpg folder which contains
                        images
  --epochs1 EPOCHS1
  --epochs2 EPOCHS2
  --batch_size BATCH_SIZE
  --lrG1 LRG1
  --lrG2 LRG2
  --lrD1 LRD1
  --lrD2 LRD2
  --beta1 BETA1
  --beta2 BETA2
```

## Contributed by:
* [Rishabh Dugaye](https://github.com/rishabhd786)

## References

* **Title**: StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks
* **Authors**: Han Zhang,Tao Xu,Hongsheng Li,Shaoting Zhang,Xiaogang Wang,Xiaolei Huang,Dimitris Metaxas
* **Link**: https://arxiv.org/pdf/1612.03242.pdf
* **Year**: 2017

# Summary 

## Introduction

Generating photo-realistic images from text is an important problem and has tremendous applications, including photo-editing, computer-aided design, etc. Recently,Generative Adversarial Networks (GAN) have
shown promising results in synthesizing real-world images. Conditioned on given text descriptions, conditional GANs are able to generate images that are highly related to the text meanings.
However, it is very difficult to train GAN to generate high-resolution photo-realistic images from text descriptions.

In analogy to how human painters draw, we decompose the problem of text to photo-realistic image synthesis into two more tractable sub-problems with Stacked Generative Adversarial Networks (StackGAN). Low-resolution images are first generated by our Stage-I GAN. On the top of our Stage-I GAN, we stack Stage-II GAN to generate realistic high-resolution (e.g., 256×256) images conditioned on Stage-I results and text descriptions. By conditioning on the Stage-I result and the
text again, Stage-II GAN learns to capture the text information that is omitted by Stage-I GAN and draws more details for the object. The support of model distribution generated from a roughly aligned low-resolution image has better
probability of intersecting with the support of image distribution. This is the underlying reason why Stage-II GAN is able to generate better high-resolution images.

## GANs

Generative adversarial nets were recently introduced as a novel way to train a generative model.
They consists of two ‘adversarial’ models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training
data rather than G. Both G and D could be a non-linear mapping function, such as a multi-layer perceptron.

## Architecture of StackGAN

To generate high-resolution images with photo-realisticdetails, paper propose a simple yet effective Stacked Generative Adversarial Networks. It decomposes the text-to-image
generative process into two stages.
- Stage-I GAN: it sketches the primitive shape and basic colors of the object conditioned on the given text description, and draws the background layout from a
                random noise vector, yielding a low-resolution image.
- Stage-II GAN: it corrects defects in the low-resolution image from Stage-I and completes details of the object 
                by reading the text description again, producing a highresolution photo-realistic image.

![1](./assets/arch.png) 

